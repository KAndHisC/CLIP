{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "from dataset import build_loaders\n",
    "from clip.model import CLIP\n",
    "from tqdm.autonotebook import tqdm\n",
    "# from tqdm import tqdm_notebook as tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgMeter:\n",
    "    def __init__(self, name=\"Metric\"):\n",
    "        self.name = name\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.avg, self.sum, self.count = [0] * 3\n",
    "\n",
    "    def update(self, val, count=1):\n",
    "        self.count += count\n",
    "        self.sum += val * count\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __repr__(self):\n",
    "        text = f\"{self.name}: {self.avg:.4f}\"\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group[\"lr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_cross_entropy_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def cross_entropy(preds, targets, reduction='none'):\n",
    "    log_softmax = nn.LogSoftmax(dim=-1)\n",
    "    loss = (-targets * log_softmax(preds)).sum(1)\n",
    "    if reduction == 'none':\n",
    "        return loss\n",
    "    elif reduction == 'mean':\n",
    "        return loss.mean()\n",
    "\n",
    "def paper_loss(logits_per_image, logits_per_text):\n",
    "    labels = torch.Tensor(np.arange(logits_per_image.size()[0])).long().to(cfg.device)\n",
    "    # labels = torch.eye(logits_per_image.size()[0]).cuda()\n",
    "\n",
    "    loss_text = torch_cross_entropy_func(logits_per_text, labels)\n",
    "    loss_image = torch_cross_entropy_func(logits_per_image, labels)\n",
    "    loss =  (loss_text + loss_image) / 2.0\n",
    "\n",
    "    return loss.mean()\n",
    "\n",
    "def custom_loss(logits_per_image, logits_per_text):\n",
    "    \n",
    "    logits = logits_per_text\n",
    "    targets = F.softmax(\n",
    "        (logits_per_image + logits_per_text) / 2, dim=-1\n",
    "    )\n",
    "\n",
    "    texts_loss = cross_entropy(logits, targets, reduction='none')\n",
    "    images_loss = cross_entropy(logits.t(), targets.t(), reduction='none')\n",
    "\n",
    "    loss = (images_loss + texts_loss) / 2.0\n",
    "\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_epoch(model, valid_loader):\n",
    "    loss_meter = AvgMeter()\n",
    "\n",
    "    tqdm_object = tqdm(valid_loader, total=len(valid_loader))\n",
    "    for images, texts in tqdm_object:\n",
    "        images = images.to(cfg.device)\n",
    "        texts = texts.to(cfg.device)\n",
    "\n",
    "        logits_per_image, logits_per_text = model(images, texts)\n",
    "        # loss = custom_loss(logits_per_image, logits_per_text)\n",
    "        loss = paper_loss(logits_per_image, logits_per_text)\n",
    "        count = images.size(0)\n",
    "        loss_meter.update(loss.item(), count)\n",
    "\n",
    "        tqdm_object.set_postfix(valid_loss=loss_meter.avg)\n",
    "\n",
    "    return loss_meter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_epoch(model, train_loader, optimizer, lr_scheduler, step):\n",
    "def train_epoch(model, train_loader, optimizer):\n",
    "    # loss_meter = AvgMeter()\n",
    "    # lr_meter = AvgMeter()\n",
    "    tqdm_object = tqdm(train_loader, total=len(train_loader))\n",
    "    i = 0\n",
    "    for images, texts in tqdm_object:\n",
    "        \n",
    "        images = images.to(cfg.device)\n",
    "        texts = texts.to(cfg.device)\n",
    "\n",
    "        logits_per_image, logits_per_text = model(images, texts)\n",
    "        # loss = custom_loss(logits_per_image, logits_per_text)\n",
    "        loss = paper_loss(logits_per_image, logits_per_text)\n",
    "        loss.backward()\n",
    "        if (i+1) % cfg.gradient_accumulation == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            scheduler.step()\n",
    "            # tqdm_object.set_postfix(train_loss=loss_meter.avg, learing_rate=cfg.lr)\n",
    "            tqdm_object.set_postfix(train_loss=loss.item(), learing_rate=scheduler.get_last_lr()[0])\n",
    "            wandb.log({\"LR\": scheduler.get_last_lr()[0], \"Loss\": loss})\n",
    "        i += 1\n",
    "        # loss_meter.update(loss.item(), count)\n",
    "        # lr_meter.update(scheduler.get_last_lr()[0], count)\n",
    "\n",
    "    # return loss_meter\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2094054/2094054 [00:02<00:00, 746649.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of image-text pairs:  2094054\n",
      "train_size:  1675244\n",
      "test_size:  418810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fsx/home/takiw/env/miniconda3/envs/clip/lib/python3.6/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    }
   ],
   "source": [
    "from config import CFG\n",
    "# config\n",
    "cfg = CFG()\n",
    "\n",
    "model = CLIP(embed_dim=cfg.embed_dim, \n",
    "            # vision\n",
    "            image_resolution=cfg.image_resolution,\n",
    "            vision_layers=cfg.vision_layers,\n",
    "            vision_width=cfg.vision_width,\n",
    "            vision_patch_size=cfg.vision_patch_size,\n",
    "            context_length=cfg.context_length,\n",
    "            vocab_size=cfg.vocab_size,\n",
    "            # text\n",
    "            transformer_width=cfg.transformer_width,\n",
    "            transformer_heads=cfg.transformer_heads,\n",
    "            transformer_layers=cfg.transformer_layers).to(cfg.device)\n",
    "\n",
    "# DataLoader\n",
    "train_loader, test_loader = build_loaders(cfg=cfg)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=cfg.lr, betas=cfg.adam_beta, eps=1e-6, weight_decay=cfg.weight_decay)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr, betas=cfg.adam_beta, eps=cfg.eps, weight_decay=cfg.weight_decay) # in paper\n",
    "\n",
    "# lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#     optimizer, mode=\"min\", patience=cfg.patience, factor=cfg.factor\n",
    "# )\n",
    "warmup_steps = cfg.warmup_epochs * len(train_loader) // cfg.gradient_accumulation\n",
    "num_steps = cfg.epochs * len(train_loader) // cfg.gradient_accumulation + 1\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, num_steps, last_epoch=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkurokawa\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.6 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.1<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">restful-pine-2</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/kurokawa/CLIP-GPU\" target=\"_blank\">https://wandb.ai/kurokawa/CLIP-GPU</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/kurokawa/CLIP-GPU/runs/3fzuvyne\" target=\"_blank\">https://wandb.ai/kurokawa/CLIP-GPU/runs/3fzuvyne</a><br/>\n",
       "                Run data is saved locally in <code>/localdata/workspace/CLIP/wandb/run-20211028_030541-3fzuvyne</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(project=\"CLIP-GPU\", settings=wandb.Settings(console='off'))\n",
    "wandb.config.update(vars(cfg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4076b37877246758c9d823dbe6089d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/119661 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fsx/home/takiw/env/miniconda3/envs/clip/lib/python3.6/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "/fsx/home/takiw/env/miniconda3/envs/clip/lib/python3.6/site-packages/PIL/TiffImagePlugin.py:811: UserWarning: Corrupt EXIF data.  Expecting to read 12 bytes but only got 8. \n",
      "  warnings.warn(str(msg))\n",
      "/fsx/home/takiw/env/miniconda3/envs/clip/lib/python3.6/site-packages/PIL/Image.py:974: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/fsx/home/takiw/env/miniconda3/envs/clip/lib/python3.6/site-packages/PIL/TiffImagePlugin.py:811: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n",
      "/fsx/home/takiw/env/miniconda3/envs/clip/lib/python3.6/site-packages/PIL/TiffImagePlugin.py:811: UserWarning: Corrupt EXIF data.  Expecting to read 12 bytes but only got 10. \n",
      "  warnings.warn(str(msg))\n",
      "/fsx/home/takiw/env/miniconda3/envs/clip/lib/python3.6/site-packages/PIL/Image.py:2914: DecompressionBombWarning: Image size (98130452 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  DecompressionBombWarning,\n",
      "/fsx/home/takiw/env/miniconda3/envs/clip/lib/python3.6/site-packages/PIL/Image.py:974: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/fsx/home/takiw/env/miniconda3/envs/clip/lib/python3.6/site-packages/PIL/JpegImagePlugin.py:812: UserWarning: Image appears to be a malformed MPO file, it will be interpreted as a base JPEG file\n",
      "  \"Image appears to be a malformed MPO file, it will be \"\n",
      "/fsx/home/takiw/env/miniconda3/envs/clip/lib/python3.6/site-packages/PIL/TiffImagePlugin.py:811: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "/fsx/home/takiw/env/miniconda3/envs/clip/lib/python3.6/site-packages/PIL/Image.py:2914: DecompressionBombWarning: Image size (136901120 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  DecompressionBombWarning,\n",
      "/fsx/home/takiw/env/miniconda3/envs/clip/lib/python3.6/site-packages/PIL/Image.py:2914: DecompressionBombWarning: Image size (93950400 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  DecompressionBombWarning,\n",
      "/fsx/home/takiw/env/miniconda3/envs/clip/lib/python3.6/site-packages/PIL/Image.py:2914: DecompressionBombWarning: Image size (107736028 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  DecompressionBombWarning,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bc2ad63af084bcc8b1fecf98f6c6d9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29915 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_loss = float('inf')\n",
    "for epoch in range(cfg.epochs):\n",
    "    print(f\"Epoch: {epoch + 1}\")\n",
    "    model.train()\n",
    "    # train_loss = train_epoch(model, train_loader, optimizer, lr_scheduler, step)\n",
    "    train_loss = train_epoch(model, train_loader, optimizer)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = valid_epoch(model, test_loader)\n",
    "\n",
    "    if valid_loss.avg < best_loss:\n",
    "        best_loss = valid_loss.avg\n",
    "        torch.save(model.state_dict(), \"./models/2m\"+str(valid_loss.avg)[:5]+\".pt\")\n",
    "        print(\"saved the best model! \")\n",
    "\n",
    "    # lr_scheduler.step(valid_loss.avg)\n",
    "    # print(\"lr: \", get_lr(optimizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dbdf44c3cb1c202bcde82d270ec00c561ec790b1cd8e864021d409a0c070b1bd"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('clip': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
